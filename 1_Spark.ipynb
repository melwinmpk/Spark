{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e0fa1",
   "metadata": {},
   "source": [
    "<h3>What is Spark? (It is Distributed Data Processing Framework)</h3>\n",
    "<ul>\n",
    "<li>Spark is <b>lightning-fast cluster computing technology</b>, designed for fast computation.</li>\n",
    "<li>Spark is primarily based on Hadoop, supports earlier model to work efficiently and offer several new computations such as <b>interactive queries</b> and <b>stream processing</b>.</li>\n",
    "<li>The main feature of Spark is its <b>in-memory cluster computing</b> that increases the processing speed of an application</li>\n",
    "<li>Spark supports high-level APIs such as Java, Scala, Python and R. It is basically built upon Scala language.</li>\n",
    "<li>Spark implements the processing around <b>10  times faster in Disk and 100 times faster in-memory than HDFS</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7fd22",
   "metadata": {},
   "source": [
    "<h3>What are Interactive Queries that are supported by Spark?</h3>\n",
    "<p>An interactive query system is basically a user interface that translates the input from the user into spark SQL queries.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90c493",
   "metadata": {},
   "source": [
    "<h3>What is Stream Processing?</h3>\n",
    "<p>Stream processing is the practice of taking action on a series of data at the time the data is created.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c6bb0",
   "metadata": {},
   "source": [
    "<h3>What is In-Memory Cluster Computing?</h3>\n",
    "<p>In-memory cluster computation enables Spark to run iterative algorithms, as programs can checkpoint data and refer back to it without reloading it from disk.</p>\n",
    "<p>\n",
    "for example:<br>\n",
    "Letâ€™s say there are 5 MR jobs i.e. MR1, MR2, MR3, MR4 & MR5.<br> So MR1 brings data from HDFS to Memory then process the data and sends the result back to disk. <br> So total 10 disk seeks i.e. 5 for Read & 5 for Write are required in this case. <br><br>\n",
    "Spark brings all data in memory from the disk, process the data and sends back the result to disk.<br> So 2 disk seeks are required i.e. 1 for Read & 1 for Write.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dde05",
   "metadata": {},
   "source": [
    "<h2>SPARK ECHO SYSTEM</h2>\n",
    "<img src=\"https://i.ytimg.com/vi/QMHVcZFr83w/hqdefault.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24e12a",
   "metadata": {},
   "source": [
    "<p>\n",
    "So, the Spark ecosystem is designed in two layers.<br>\n",
    "The bottom layer is the Spark Core Layer.<br>\n",
    "Then we have the next layer, a set of DSLs, libraries, and APIs.<br>\n",
    "These two things together make the Spark ecosystem.<br>\n",
    "The Spark core layer itself has got two parts.<br>\n",
    "A distributed Computing Engine And a set of core APIs<br>\n",
    "And this whole thing runs on a cluster of computers to offer you distributed data processing.<br>\n",
    "However, Spark does not manage the cluster.<br>\n",
    "It only gives you a data processing framework.<br>\n",
    "So, you are going to need a cluster manager.<br>\n",
    "People use different names for Cluster Manager, such as Resource Manager or the Container Orchestrator.<br>\n",
    "However, they all offer similar services to Spark, and Spark is designed to work with both.<br>\n",
    "Spark was initially based on Hadoop MR, and it was adopted in the Hadoop platform.<br>\n",
    "So, the Hadoop YARN resource manager is the most commonly used cluster manager for Spark.<br>\n",
    "However, you have other options, such as Mesos and Spark Standalone Cluster Manager.<br>\n",
    "The newer versions of Spark are also compatible with Kubernetes as a Cluster Orchestrator.<br>\n",
    "Similarly, Spark also doesn't come with an in-built storage system.<br>\n",
    "And it allows you to process the data, which is stored in a variety of storage systems.<br>\n",
    "The most popular and commonly used storage systems are<br>\n",
    "HDFS, Amazon S3, Azure Data Lase Storage, Google Cloud Storage, and the Cassandra file system.<br>\n",
    "So, one thing is obvious.<br>\n",
    "Apache Spark does not offer Cluster Management and Storage Management.<br>\n",
    "All you can do with Apache Spark is to run your data processing workload.<br>\n",
    "And that part is managed by the Spark Compute Engine.    <br>\n",
    "So the compute engine is responsible for a bunch of things.<br>\n",
    "\n",
    "For example, breaking your data processing work into smaller tasks,<br>\n",
    "scheduling those tasks on the cluster for parallel execution, providing data to these tasks,<br>\n",
    "managing and monitoring those tasks, provide you fault tolerance when a job fails.<br>\n",
    "And to do all these,<br>\n",
    "the core engine is also responsible for interacting with the cluster manager and the data storage manager.<br>\n",
    "So the Spark compute engine is the core that runs and manages your data processing work<br>\n",
    "and provides you with a seamless experience.<br>\n",
    "All you need to do is submit your data processing jobs to Spark, <br>\n",
    "and the Spark core will take care of everything else.<br>\n",
    "Now let's move a level up.<br>\n",
    "The second part of the Spark Core<br>\n",
    "The Core APIs.<br>\n",
    "    \n",
    "<p>This layer is the programming interface layer that offers you the core APIs in four major languages.</p>\n",
    "<ul>\n",
    "<li>Scala,</li>\n",
    "<li>Java,</li>\n",
    "<li>Python,</li>\n",
    "<li>and R programming language.</li>\n",
    "</ul>    \n",
    "\n",
    "These are the APIs that we used to write data processing logic during the initial days of Apache Spark.<br>\n",
    "However, these APIs were based on resilient distributed datasets (RDD).<br>\n",
    "These APIs can offer you the highest level of flexibility to solve some complex data processing problems. (at present Dataframes are recommended over Rdd's)<br>\n",
    "<p>\n",
    "So, you will be using these top-level APIs and DSLs.<br>\n",
    "But internally, all of those will be using Spark Core APIs, and ultimately things will go to the Spark Compute Engine.<br>\n",
    "The topmost API layer is grouped into four categories to support four different data processing requirements.<br>\n",
    "However, this is just a logical grouping, and there is no rigid boundary.<br>\n",
    "Things are going to overlap in most of the real-life projects.<br>\n",
    "The first group is a set of two things.<br>\n",
    "Spark SQL and then Spark DataFrame/Dataset APIs<br>\n",
    "Spark SQL allows you to use SQL queries to process your data.<br>\n",
    "So that part is quite simple for those who already know SQL.<br>\n",
    "Spark DataFrame/DataSet will allow you to use functional programming techniques<br>\n",
    "to solve your data crunching problems.<br>\n",
    "These APIs are available in Java, Scala, and Python.<br>\n",
    "Both of these together can help you resolve most of the structured and semistructured data crunching problems.<br>\n",
    "The next one is Spark Streaming libraries,<br>\n",
    "and they allow you to process a continuous and unbounded stream of data<br>\n",
    "Then you have a set of libraries specifically designed to meet your machine learning,<br>\n",
    "deep learning, and AI requirements.<br>\n",
    "The last collection is for Graph Processing libraries,<br>\n",
    "and they allow you to implement Graph Processing Algorithms using Apache Spark.<br>\n",
    "So the topmost layer is nothing but a set of libraries and DSLs to help you solve your data crunching problems.<br>\n",
    "And all these are available in multiple languages such as Java, Scala, Python, and R.<br>\n",
    "    </p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83068e95",
   "metadata": {},
   "source": [
    "<p>Melwin Remember </p>\n",
    "<p><b>Apache Spark does not support the Cluster Management and Storage Management </b></p>\n",
    "<p><b>Apache Spark supports data processing it is done by Spark engine</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950dea0",
   "metadata": {},
   "source": [
    "<h3>Spark Engin functionality</h3>\n",
    "<ul>\n",
    "    <li>Breaking the Data Processing work into small task</li>\n",
    "    <li>Schedule those tasks on the cluster</li>\n",
    "    <li>Parallel Execution</li>\n",
    "    <li>Providing data for each task</li>\n",
    "    <li>It helps in Interacting with the Cluster Manager and Resource Manager</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287e550",
   "metadata": {},
   "source": [
    "<h3>Spark Core</h3>\n",
    "<p>The Programming Interface Layer offers 4 core APIs => Scala, Python, Java, R</p>\n",
    "<p>These API's were based on Resilient Distributed Dataset (RDD)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf2682",
   "metadata": {},
   "source": [
    "<h2>Spark Architecture</h2>\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8415c5e",
   "metadata": {},
   "source": [
    "<p>Spark uses <b>master/slave architecture</b>. As you can see in the figure,<br> it has one central\n",
    "coordinator (Driver) that communicates with many distributed workers (executors).<br>\n",
    "The driver and each of the executors run in their own JVM. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8733ca0",
   "metadata": {},
   "source": [
    "<h3>DRIVER</h3>\n",
    "<p>\n",
    "The driver is the process where the main method runs.<br> First it converts the user\n",
    "program into tasks and after that it schedules the tasks on the executors. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87911e8d",
   "metadata": {},
   "source": [
    "<h3>EXECUTORS</h3>\n",
    "<p>Executors are worker nodes processes in charge of running individual tasks in a given\n",
    "Spark job.<br> They are launched at the beginning of a Spark application and typically run\n",
    "for the entire lifetime of an application.<br> Once they have run the task they send the\n",
    "results to the driver. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335ee5e",
   "metadata": {},
   "source": [
    "<h3>Execution Flow of Spark Application</h3>\n",
    "<ol>\n",
    "<!-- <li></li> -->\n",
    "<li>Client submits user application code. When an application is submitted the driver\n",
    "implicitly converts the application code containing transformations & actions into a\n",
    "<b>DAG</b>. </li>\n",
    "<li>At this stage it performs pipeline optimization by resolving <b>Unresolved Logical\n",
    "Plans into a Physical Execution Plan</b> which contains jobs, stages & tasks. </li>\n",
    "<li>Now the driver talks to Cluster Manager & negotiates for resources. CM launches\n",
    "executors on worker nodes on behalf of the driver</li>\n",
    "<li>Now the driver sends the tasks to these executors based on data placement. </li>\n",
    "<li>When executor starts they register themselves with drivers so that the driver will\n",
    "have complete view of all executors</li>\n",
    "<li>Executors starts executing the task assigned by the driver & will be monitored by\n",
    "your driver program. </li>\n",
    "<li>Driver schedules future tasks. Tracks the location of cached data to schedule future\n",
    "tasks. </li>\n",
    "<li>Driver provides all of the above information of running application on Spark Web\n",
    "UI on port</li>\n",
    "<li>When the driverâ€™s sc stop method is called it will terminate all the executors &\n",
    "release resources from CM</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4b6c9",
   "metadata": {},
   "source": [
    "<img src=\"https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2019/11/Internals-of-Job-Execution-In-Spark.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1531e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
