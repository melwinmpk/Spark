{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7e0fa1",
   "metadata": {},
   "source": [
    "<h3>What is Spark? (It is Distributed Data Processing Framework)</h3>\n",
    "<ul>\n",
    "<li>Spark is <b>lightning-fast cluster computing technology</b>, designed for fast computation.</li>\n",
    "<li>Spark is primarily based on Hadoop, supports earlier model to work efficiently and offer several new computations such as <b>interactive queries</b> and <b>stream processing</b>.</li>\n",
    "<li>The main feature of Spark is its <b>in-memory cluster computing</b> that increases the processing speed of an application</li>\n",
    "<li>Spark supports high-level APIs such as Java, Scala, Python and R. It is basically built upon Scala language.</li>\n",
    "<li>Spark implements the processing around <b>10  times faster in Disk and 100 times faster in-memory than HDFS</b></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7fd22",
   "metadata": {},
   "source": [
    "<h3>What are Interactive Queries that are supported by Spark?</h3>\n",
    "<p>An interactive query system is basically a user interface that translates the input from the user into spark SQL queries.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90c493",
   "metadata": {},
   "source": [
    "<h3>What is Stream Processing?</h3>\n",
    "<p>Stream processing is the practice of taking action on a series of data at the time the data is created.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c6bb0",
   "metadata": {},
   "source": [
    "<h3>What is In-Memory Cluster Computing?</h3>\n",
    "<p>In-memory cluster computation enables Spark to run iterative algorithms, as programs can checkpoint data and refer back to it without reloading it from disk.</p>\n",
    "<p>\n",
    "for example:<br>\n",
    "Let’s say there are 5 MR jobs i.e. MR1, MR2, MR3, MR4 & MR5.<br> So MR1 brings data from HDFS to Memory then process the data and sends the result back to disk. <br> So total 10 disk seeks i.e. 5 for Read & 5 for Write are required in this case. <br><br>\n",
    "Spark brings all data in memory from the disk, process the data and sends back the result to disk.<br> So 2 disk seeks are required i.e. 1 for Read & 1 for Write.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83dde05",
   "metadata": {},
   "source": [
    "<h2>SPARK ECHO SYSTEM</h2>\n",
    "<img src=\"https://i.ytimg.com/vi/QMHVcZFr83w/hqdefault.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83068e95",
   "metadata": {},
   "source": [
    "<p>Melwin Remember </p>\n",
    "<p><b>Apache Spark does not support the Cluster Management and Storage Management </b></p>\n",
    "<p><b>Apache Spark supports data processing it is done by Spark engine</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950dea0",
   "metadata": {},
   "source": [
    "<h3>Spark Engin functionality</h3>\n",
    "<ul>\n",
    "    <li>Breaking the Data Processing work into small task</li>\n",
    "    <li>Schedule those tasks on the cluster</li>\n",
    "    <li>Parallel Execution</li>\n",
    "    <li>Providing data for each task</li>\n",
    "    <li>It helps in Interacting with the Cluster Manager and Resource Manager</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e287e550",
   "metadata": {},
   "source": [
    "<h3>Spark Core</h3>\n",
    "<p>The Programming Interface Layer offers 4 core APIs => Scala, Python, Java, R</p>\n",
    "<p>These API's were based on Resilient Distributed Dataset (RDD)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caf2682",
   "metadata": {},
   "source": [
    "<h2>Spark Architecture</h2>\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/cluster-overview.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8415c5e",
   "metadata": {},
   "source": [
    "<p>Spark uses <b>master/slave architecture</b>. As you can see in the figure,<br> it has one central\n",
    "coordinator (Driver) that communicates with many distributed workers (executors).<br>\n",
    "The driver and each of the executors run in their own JVM. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8733ca0",
   "metadata": {},
   "source": [
    "<h3>DRIVER</h3>\n",
    "<p>\n",
    "The driver is the process where the main method runs.<br> First it converts the user\n",
    "program into tasks and after that it schedules the tasks on the executors. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87911e8d",
   "metadata": {},
   "source": [
    "<h3>EXECUTORS</h3>\n",
    "<p>Executors are worker nodes processes in charge of running individual tasks in a given\n",
    "Spark job.<br> They are launched at the beginning of a Spark application and typically run\n",
    "for the entire lifetime of an application.<br> Once they have run the task they send the\n",
    "results to the driver. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335ee5e",
   "metadata": {},
   "source": [
    "<h3>Execution Flow of Spark Application</h3>\n",
    "<ol>\n",
    "<!-- <li></li> -->\n",
    "<li>Client submits user application code. When an application is submitted the driver\n",
    "implicitly converts the application code containing transformations & actions into a\n",
    "<b>DAG</b>. </li>\n",
    "<li>At this stage it performs pipeline optimization by resolving <b>Unresolved Logical\n",
    "Plans into a Physical Execution Plan</b> which contains jobs, stages & tasks. </li>\n",
    "<li>Now the driver talks to Cluster Manager & negotiates for resources. CM launches\n",
    "executors on worker nodes on behalf of the driver</li>\n",
    "<li>Now the driver sends the tasks to these executors based on data placement. </li>\n",
    "<li>When executor starts they register themselves with drivers so that the driver will\n",
    "have complete view of all executors</li>\n",
    "<li>Executors starts executing the task assigned by the driver & will be monitored by\n",
    "your driver program. </li>\n",
    "<li>Driver schedules future tasks. Tracks the location of cached data to schedule future\n",
    "tasks. </li>\n",
    "<li>Driver provides all of the above information of running application on Spark Web\n",
    "UI on port</li>\n",
    "<li>When the driver’s sc stop method is called it will terminate all the executors &\n",
    "release resources from CM</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e4b6c9",
   "metadata": {},
   "source": [
    "<img src=\"https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2019/11/Internals-of-Job-Execution-In-Spark.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1531e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
