{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bc414c",
   "metadata": {},
   "source": [
    "<h3>What is SparkContext in Spark?</h3>\n",
    "<ul>\n",
    "    <li>SparkContext is the entry point of Apache Spark functionality. </li>\n",
    "    <li>The most important step of any driver application is to generate <b>SparkContext</b></li>\n",
    "    <li>It allows your application to access <b>cluster</b> with the help of <b>Resource Manager</b>. </li>\n",
    "    <li>To create <b>SparkContext</b>, first <b>SparkConf</b> should be made. </li>\n",
    "    <li>The <b>SparkConf</b> has a configuration parameter that our driver application will pass\n",
    "to SparkContext</li>\n",
    "</ul>\n",
    "<img src=\"https://cdn.educba.com/academy/wp-content/uploads/2020/10/How-Apache-Spark-Context-is-created.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cbfe6",
   "metadata": {},
   "source": [
    "<h3>what are RDD's ?</h3>\n",
    "<p><b>Resilient Distributed Datasets (RDD)</b> is a fundamental data structure of Spark. It is an immutable distributed collection of objects</p>\n",
    "<p>Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ce378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
