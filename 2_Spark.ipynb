{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bc414c",
   "metadata": {},
   "source": [
    "<h3>What is SparkContext in Spark?</h3>\n",
    "<ul>\n",
    "    <li>SparkContext is the entry point of Apache Spark functionality. </li>\n",
    "    <li>The most important step of any driver application is to generate <b>SparkContext</b></li>\n",
    "    <li>It allows your application to access <b>cluster</b> with the help of <b>Resource Manager</b>. </li>\n",
    "    <li>To create <b>SparkContext</b>, first <b>SparkConf</b> should be made. </li>\n",
    "    <li>The <b>SparkConf</b> has a configuration parameter that our driver application will pass\n",
    "to SparkContext</li>\n",
    "</ul>\n",
    "<img src=\"https://cdn.educba.com/academy/wp-content/uploads/2020/10/How-Apache-Spark-Context-is-created.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e3b21",
   "metadata": {},
   "source": [
    "<h3>what are RDD's ?</h3>\n",
    "<ul>\n",
    "<li><b>Resilient Distributed Dataset (aka RDD)</b> is the primary data abstraction in\n",
    "Apache Spark and the core of Spark i.e. referred as \"Spark Core\".</li>\n",
    "<li>It is immutable collection of objects & <b>lazily evaluated</b>. </li>\n",
    "<li>Each dataset in RDD is divided into logical partitions, which may be computed\n",
    "on different nodes of the cluster.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3a173",
   "metadata": {},
   "source": [
    "<h3>RDD Benefits</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451b08d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>In-Memory Processing</b></li>\n",
    "<li><b>Immutability</b></li>\n",
    "<li><b>Fault Tolerance:</b><br>\n",
    "    RDDs are fault tolerant as they track <b>data lineage</b> information to rebuild lost data automatically on failure</li>\n",
    "<li><b>Lazy Evolution</b></li>\n",
    "<li><b>Partitioning<b></li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbedfe",
   "metadata": {},
   "source": [
    "<h3>RDD Limitations</h3>\n",
    "<ul>\n",
    "<li>Spark RDDs are not much suitable for applications that make updates to the state\n",
    "store such as storage systems for a web application. </li>\n",
    "<li>For these applications, it is more efficient to use systems that perform traditional\n",
    "update, logging and data checkpointing such as databases. </li>\n",
    "<li>The goal of RDD is to provide an efficient programming model for batch analytics.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65148fb0",
   "metadata": {},
   "source": [
    "<h3>Spark RDD Operations</h3>\n",
    "<ul>\n",
    "    <li>Transformation</li>\n",
    "    <li>Actions</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d9a2a",
   "metadata": {},
   "source": [
    "<h3>Type of Transformation</h3>\n",
    "<ul>\n",
    "    <li>Narrow Transformations</li>\n",
    "    <li>Wide Transformations</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf402d5f",
   "metadata": {},
   "source": [
    "<h4>Narrow Transformations</h4>\n",
    "<p> all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f77bf",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/melwinmpk/Spark/blob/main/img/Narrow_Transformation.PNG?raw=true\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448737e",
   "metadata": {},
   "source": [
    "<h4>Wide Transformations</h4>\n",
    "<p>In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey() and reducebyKey().</p>\n",
    "\n",
    "<img src=\"https://github.com/melwinmpk/Spark/blob/main/img/Wide_Transformation.PNG?raw=true\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a661e",
   "metadata": {},
   "source": [
    "<h3>Spark difference between reduceByKey vs. groupByKey vs. aggregateByKey vs. combineByKey</h3>\n",
    "<a href=\"https://stackoverflow.com/questions/43364432/spark-difference-between-reducebykey-vs-groupbykey-vs-aggregatebykey-vs-combi#:~:text=groupByKey%20can%20cause%20out%20of,collected%20on%20the%20reduced%20workers.&text=Data%20are%20combined%20at%20each,with%20the%20exact%20same%20type\" target=\"_blank\">LINK</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89810f9",
   "metadata": {},
   "source": [
    "<h3>Is groupByKey ever preferred over reduceByKey ?</h3>\n",
    "<a href=\"https://stackoverflow.com/questions/33221713/is-groupbykey-ever-preferred-over-reducebykey\" target=\"_blank\">Link</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7542ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
