{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97bc414c",
   "metadata": {},
   "source": [
    "<h3>What is SparkContext in Spark?</h3>\n",
    "<ul>\n",
    "    <li>SparkContext is the entry point of Apache Spark functionality. </li>\n",
    "    <li>The most important step of any driver application is to generate <b>SparkContext</b></li>\n",
    "    <li>It allows your application to access <b>cluster</b> with the help of <b>Resource Manager</b>. </li>\n",
    "    <li>To create <b>SparkContext</b>, first <b>SparkConf</b> should be made. </li>\n",
    "    <li>The <b>SparkConf</b> has a configuration parameter that our driver application will pass\n",
    "to SparkContext</li>\n",
    "</ul>\n",
    "<img src=\"https://cdn.educba.com/academy/wp-content/uploads/2020/10/How-Apache-Spark-Context-is-created.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f5880",
   "metadata": {},
   "source": [
    "### what are Spark Execution Modes ?\n",
    "<ul>\n",
    "    <li>Client (Notebook, Spark shell )</li>\n",
    "    <li>Cluster (spark submit)</li>\n",
    "</ul>\n",
    "\n",
    "<img src=\"https://www.learningjournal.guru/_resources/img/jpg-7x/spark-yarn-resource-allocation-in-client-mode.jpg\"></img>\n",
    "<img  src=\"https://qph.fs.quoracdn.net/main-qimg-c05cf0061ee8d4b3c23caabf9b7890c3\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e3b21",
   "metadata": {},
   "source": [
    "<h3>what are RDD's ?</h3>\n",
    "<ul>\n",
    "<li><b>Resilient Distributed Dataset (aka RDD)</b> is the primary data abstraction in\n",
    "Apache Spark and the core of Spark i.e. referred as \"Spark Core\".</li>\n",
    "<li>It is immutable collection of objects & <b>lazily evaluated</b>. </li>\n",
    "<li>Each dataset in RDD is divided into logical partitions, which may be computed\n",
    "on different nodes of the cluster.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3a173",
   "metadata": {},
   "source": [
    "<h3>RDD Benefits</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451b08d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>In-Memory Processing</b></li>\n",
    "<li><b>Immutability</b></li>\n",
    "<li><b>Fault Tolerance:</b><br>\n",
    "    RDDs are fault tolerant as they track <b>data lineage</b> information to rebuild lost data automatically on failure</li>\n",
    "<li><b>Lazy Evolution</b></li>\n",
    "<li><b>Partitioning<b></li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efbedfe",
   "metadata": {},
   "source": [
    "<h3>RDD Limitations</h3>\n",
    "<ul>\n",
    "<li>Spark RDDs are not much suitable for applications that make updates to the state\n",
    "store such as storage systems for a web application. </li>\n",
    "<li>For these applications, it is more efficient to use systems that perform traditional\n",
    "update, logging and data checkpointing such as databases. </li>\n",
    "<li>The goal of RDD is to provide an efficient programming model for batch analytics.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65148fb0",
   "metadata": {},
   "source": [
    "<h3>Spark RDD Operations</h3>\n",
    "<ul>\n",
    "    <li>Transformation</li>\n",
    "    <li>Actions</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d9a2a",
   "metadata": {},
   "source": [
    "<h3>Type of Transformation</h3>\n",
    "<ul>\n",
    "    <li>Narrow Transformations</li>\n",
    "    <li>Wide Transformations</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf402d5f",
   "metadata": {},
   "source": [
    "<h4>Narrow Transformations</h4>\n",
    "<p> all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5f77bf",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/melwinmpk/Spark/main/img/Narrow_Transformation.PNG?token=GHSAT0AAAAAABPTTAFGQW3WUHLVAXSR54WIYQPDEZA\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448737e",
   "metadata": {},
   "source": [
    "<h4>Wide Transformations</h4>\n",
    "<p>In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey() and reducebyKey().</p>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/melwinmpk/Spark/main/img/Wide_Transformation.PNG?token=GHSAT0AAAAAABPTTAFHRPGHYLHWCPXKFVTYYQPDFKA\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a661e",
   "metadata": {},
   "source": [
    "<h3>Spark difference between reduceByKey vs. groupByKey vs. aggregateByKey vs. combineByKey</h3>\n",
    "<a href=\"https://stackoverflow.com/questions/43364432/spark-difference-between-reducebykey-vs-groupbykey-vs-aggregatebykey-vs-combi#:~:text=groupByKey%20can%20cause%20out%20of,collected%20on%20the%20reduced%20workers.&text=Data%20are%20combined%20at%20each,with%20the%20exact%20same%20type\" target=\"_blank\">LINK</a>\n",
    "<h3>Is groupByKey ever preferred over reduceByKey ?</h3>\n",
    "<a href=\"https://stackoverflow.com/questions/33221713/is-groupbykey-ever-preferred-over-reducebykey\" target=\"_blank\">Link</a>\n",
    "<img src=\"https://www.edureka.co/community/?qa=blob&qa_blobid=6565348686735863167\"></img>\n",
    "<img src=\"https://www.edureka.co/community/?qa=blob&qa_blobid=8024890559746280233\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e913ba",
   "metadata": {},
   "source": [
    "### Actions\n",
    "<ul>\n",
    "    <li>When action is triggered new RDD is not formed like transformations. Thus,\n",
    "actions are operation that gives non-RDD values.</li>\n",
    "    <li>The values of action are sent to drivers or to the external storage system</li>\n",
    "    <li>It brings laziness of RDD into motion. </li>\n",
    "    <li>An action is one of the ways of sending data from executer to the driver. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bac1e",
   "metadata": {},
   "source": [
    "<h3>Action Operations</h3>\n",
    "<ul>\n",
    "    <li>count</li>\n",
    "    <li>take</li>\n",
    "    <li>collect</li>\n",
    "    <li>top</li>\n",
    "    <li>countByValue</li>\n",
    "    <li>countByKey</li>\n",
    "    <li>reduce</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c4a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
