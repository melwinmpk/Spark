{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6fda7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Core Per Node =>  16\n",
      "Leave One Core per Node for Hadoop/Yarn decmon 16 - 1 (and RAM 1 GB) => 15 \n",
      "So Avaliable RAM is 64 GB -1 => 63\n",
      "Total Avaliable Cores in Cluster [10 x 15] => 150\n",
      "Lets Assume we go with 5 (executors valude needs to be x <= 5)\n",
      "No of Available Executors [150 / 5] => 30 \n",
      "No of Executor per Node [30/10] => 3 \n",
      "Leaving 1 Executor for Application Management [30-1] => 29 \n",
      "Memory Per executor [63/3] => 21\n",
      "Counting off Heap over head 14% approx [21 * 0.14] => 3   \n",
      "So actual executor memory [ 21 - 3 ]=> 18 \n",
      "\n",
      "spark-submit --num-executors 29 --executor-cores 5 --executor-memory 18G --conf spark.yarn.executor.memoryOverhead=3G --driver-memory 12G your_app.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "Nodes = 10\n",
    "Cores_per_Node = 16\n",
    "Ram_per_Node = 64 # in GB\n",
    "num_executors = 5 # valude needs to be x <= 5\n",
    "\n",
    "\n",
    "# Nodes = 5\n",
    "# Cores_per_Node = 8\n",
    "# Ram_per_Node = 32 # in GB\n",
    "# num_executors = 3 # valude needs to be x <= 5\n",
    "\n",
    "# Nodes = 4\n",
    "# Cores_per_Node = 12\n",
    "# Ram_per_Node = 48 # in GB\n",
    "# num_executors = 4 # valude needs to be x <= 5\n",
    "\n",
    "\n",
    "print(f\"No of Core Per Node =>  {Cores_per_Node}\")\n",
    "print(f\"Leave One Core per Node for Hadoop/Yarn decmon {Cores_per_Node} - 1 (and RAM 1 GB) => {Cores_per_Node-1} \")\n",
    "Cores_per_Node-=1\n",
    "\n",
    "print(f\"So Avaliable RAM is {Ram_per_Node} GB -1 => {Ram_per_Node-1}\")\n",
    "Ram_per_Node-=1\n",
    "\n",
    "print(f\"Total Avaliable Cores in Cluster [{Nodes} x {Cores_per_Node}] => {Nodes*(Cores_per_Node)}\")\n",
    "Total_cores = Nodes*(Cores_per_Node)\n",
    "print(f\"Lets Assume we go with {num_executors} (executors valude needs to be x <= 5)\")\n",
    "print(f\"No of Available Executors [{Total_cores} / {num_executors}] => {math.floor(Total_cores/num_executors)} \")\n",
    "Available_Executors = math.floor(Total_cores/num_executors)\n",
    "\n",
    "print(f\"No of Executor per Node [{Available_Executors}/{Nodes}] => {math.floor(Available_Executors/Nodes)} \")\n",
    "executors_per_node = math.floor(Available_Executors/Nodes)\n",
    "print(f\"Leaving 1 Executor for Application Management [{Available_Executors}-1] => {math.floor(Available_Executors-1)} \")\n",
    "no_of_executors = math.floor(Available_Executors-1)\n",
    "#executors_per_node = math.floor(Available_Executors/Nodes)\n",
    "\n",
    "print(f\"Memory Per executor [{Ram_per_Node}/{executors_per_node}] => {math.floor(Ram_per_Node/executors_per_node)}\")\n",
    "Memory_per_executor = math.floor(Ram_per_Node/executors_per_node)\n",
    "print(f\"Counting off Heap over head 14% approx [{Memory_per_executor} * 0.14] => { math.ceil(Memory_per_executor*0.14) }   \")\n",
    "head_over_heap = math.ceil(Memory_per_executor*0.14)\n",
    "print(f\"So actual executor memory [ {Memory_per_executor} - {head_over_heap} ]=> {Memory_per_executor -  head_over_heap } \")\n",
    "final_executor_memory = Memory_per_executor -  head_over_heap\n",
    "\n",
    "\n",
    "print(f'''\n",
    "spark-submit \\\n",
    "--num-executors {no_of_executors} \\\n",
    "--executor-cores {num_executors} \\\n",
    "--executor-memory {final_executor_memory }G \\\n",
    "--conf spark.yarn.executor.memoryOverhead={head_over_heap}G \\\n",
    "--driver-memory 12G \\\n",
    "your_app.py\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a2fa0",
   "metadata": {},
   "source": [
    "## Is there any logic to calculate driver memory ?\n",
    "\n",
    "there is a practical logic to estimating driver memory, although it's a bit more art than strict science. Let's break it down so you can apply it with confidence:\n",
    "\n",
    "üß† What the Driver Does\n",
    "The Spark driver:\n",
    "- Coordinates execution across the cluster\n",
    "- Maintains metadata and DAGs\n",
    "- Collects and logs results (especially in actions like .collect() or .count())\n",
    "If your job involves:\n",
    "- Heavy broadcasting\n",
    "- Large metadata (many partitions, complex DAGs)\n",
    "- Big result sets returned to the driver\n",
    "‚Üí Then it needs more memory.\n",
    "\n",
    "üìè Heuristic for Driver Memory Allocation\n",
    "There's no universal formula, but here are smart heuristics based on your executor memory and job profile:\n",
    "| Scenario | Driver Memory Tip | \n",
    "| Small to moderate jobs | Match executor memory | \n",
    "| Medium/large jobs with caching/shuffles | Executor memory + 20‚Äì50% | \n",
    "| Jobs using .collect() on big data | 2√ó executor memory or more | \n",
    "| Jobs in cluster mode (YARN/K8s) | At least 2‚Äì4 GB reserved minimum | \n",
    "\n",
    "\n",
    "üí° In your recent config:\n",
    "- Executors: 19 GB memory\n",
    "- You‚Äôve chosen: --driver-memory 12G\n",
    "That's a conservative and generally safe choice‚Äîespecially if you aren't collecting huge datasets. But if you're noticing slow performance or OOM errors on the driver, try bumping to 16‚Äì20 GB.\n",
    "\n",
    "üîß Optional Spark Tuning Flags\n",
    "You can also set:\n",
    "--conf spark.driver.memoryOverhead=2G\n",
    "\n",
    "\n",
    "If the driver is used heavily, like in streaming apps or complex joins, this ensures more buffer space beyond the heap.\n",
    "\n",
    "Want to build a small utility or config calculator that suggests driver memory based on job type and executor setup? That could be a powerful scripting aid for future deployments üí•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9ad36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
