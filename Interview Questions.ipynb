{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784b805c",
   "metadata": {},
   "source": [
    "### When to use Avro and when Parquet and Why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31137",
   "metadata": {},
   "source": [
    "<p><b>AVRO</b> is a row-based storage format whereas PARQUET is a columnar based storage format.</p>\n",
    "<p><b>PARQUET</b> is a columnar based storage format.</p>\n",
    "<p><b>PARQUET</b> is much better for analytical querying i.e. reads and querying are much more efficient than writing.<br> Write operations in <b>AVRO</b> are better than in <b>PARQUET</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb71f32",
   "metadata": {},
   "source": [
    "### Do you have any idea Hadoop Erasure in coding?\n",
    "<p>Erasure coding, a new feature in HDFS, <b>can reduce storage overhead by approximately 50% compared to replication</b> while maintaining the same durability guarantees. This post explains how it works.</p>\n",
    "\n",
    "link : https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/\n",
    "\n",
    "In Hadoop3 we can enable Erasure coding policy to any folder in HDFS. By default erasure coding is not enabled in Hadoop3\n",
    "\n",
    "### How to Configure it ?\n",
    "link https://stackoverflow.com/questions/51475712/hadoop-3-how-to-configure-enable-erasure-coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f448b26",
   "metadata": {},
   "source": [
    "### What is Spark driver?\n",
    "<p>The spark driver is that the program that defines the <b>transformations and actions on RDDs</b> of knowledge and submits request to the master. Spark driver is a program that runs on the master node of the machine which declares transformations and actions on knowledge RDDs.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3ea31",
   "metadata": {},
   "source": [
    "### What is Spark session ?\n",
    "<p>Spark session is a unified entry point of a spark application from Spark 2.0. <br> It provides a way to interact with various spark's functionality with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab2e48",
   "metadata": {},
   "source": [
    "### who will take care spark cluster resources?\n",
    "<p>Cluster Manager (Mesos, YARN or Kubernetes)</p>\n",
    "<p>Driver talks to Cluster Manager & negotiates for resources. CM launches executors on worker nodes on behalf of the driver</p>\n",
    "<p>The Spark Master is the process that requests resources in the cluster and makes them available to the Spark Driver. In all deployment modes, the Master negotiates resources or containers with Worker nodes or slave nodes and tracks their status and monitors their progress.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee555741",
   "metadata": {},
   "source": [
    "### Deployment modes: Cluster and Client?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b24f3",
   "metadata": {},
   "source": [
    "### How the client reads a file from HDFS ?\n",
    "\n",
    "<ul>\n",
    "    <li>HDFS Client: On user behalf, HDFS client interacts with NameNode and Datanode to fulfill user requests.</li>\n",
    "    <li>NameNode: NameNode is the master node that stores metadata about block locations, blocks of a file, etc. This metadata is used for file read and write operation.</li>\n",
    "    <li>DataNode: DataNodes are the slave nodes in HDFS. They store actual data (data blocks).</li>\n",
    "</ul>\n",
    " <p>HDFS client wants to read a file “File.txt”</p>\n",
    " <p>The client will reach out to NameNode asking locations of DataNodes containing data blocks. <br>\n",
    "    The NameNode first checks for required privileges, and if the client has sufficient privileges, the NameNode sends the locations of DataNodes containing blocks</p>\n",
    " <p> NameNode also gives a security token to the client, which they need to show to the DataNodes for authentication.</p>\n",
    " <a href=\"https://data-flair.training/blogs/hdfs-data-read-operation/\">How the client reads a file from HDFS Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2122cfb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77d0dfb9",
   "metadata": {},
   "source": [
    "### What types of Hadoop nodes are used by HBase?\n",
    "Masters -- HDFS NameNode, YARN ResourceManager, and HBase Master. Slaves -- HDFS DataNodes, YARN NodeManagers, <br>\n",
    "and HBase RegionServers. The DataNodes, NodeManagers, and HBase RegionServers are co-located or co-deployed <br>\n",
    "for optimal data locality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d470996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
