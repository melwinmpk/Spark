{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6345c519",
   "metadata": {},
   "source": [
    "## ** Distribution of Executors, Cores and Memory for a Spark Application running in Yarn: \n",
    "\n",
    "https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8910357a",
   "metadata": {},
   "source": [
    "## **Understanding Spark UI\n",
    "\n",
    "https://sparkbyexamples.com/spark/spark-web-ui-understanding/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b805c",
   "metadata": {},
   "source": [
    "### When to use Avro and when Parquet and Why ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31137",
   "metadata": {},
   "source": [
    "<p><b>AVRO</b> is a row-based storage format whereas PARQUET is a columnar based storage format.</p>\n",
    "<p><b>PARQUET</b> is a columnar based storage format.</p>\n",
    "<p><b>PARQUET</b> is much better for analytical querying i.e. reads and querying are much more efficient than writing.<br> Write operations in <b>AVRO</b> are better than in <b>PARQUET</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb71f32",
   "metadata": {},
   "source": [
    "### Do you have any idea Hadoop Erasure in coding?\n",
    "<p>Erasure coding, a new feature in HDFS, <b>can reduce storage overhead by approximately 50% compared to replication</b> while maintaining the same durability guarantees. This post explains how it works.</p>\n",
    "\n",
    "link : https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/\n",
    "\n",
    "In Hadoop3 we can enable Erasure coding policy to any folder in HDFS. By default erasure coding is not enabled in Hadoop3\n",
    "\n",
    "### How to Configure it ?\n",
    "link https://stackoverflow.com/questions/51475712/hadoop-3-how-to-configure-enable-erasure-coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f448b26",
   "metadata": {},
   "source": [
    "### What is Spark driver?\n",
    "<p>The spark driver is that the program that defines the <b>transformations and actions on RDDs</b> of knowledge and submits request to the master. Spark driver is a program that runs on the master node of the machine which declares transformations and actions on knowledge RDDs.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3ea31",
   "metadata": {},
   "source": [
    "### What is Spark session ?\n",
    "<p>Spark session is a unified entry point of a spark application from Spark 2.0. <br> It provides a way to interact with various spark's functionality with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab2e48",
   "metadata": {},
   "source": [
    "### who will take care spark cluster resources?\n",
    "<p>Cluster Manager (Mesos, YARN or Kubernetes)</p>\n",
    "<p>Driver talks to Cluster Manager & negotiates for resources. CM launches executors on worker nodes on behalf of the driver</p>\n",
    "<p>The Spark Master is the process that requests resources in the cluster and makes them available to the Spark Driver. In all deployment modes, the Master negotiates resources or containers with Worker nodes or slave nodes and tracks their status and monitors their progress.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee555741",
   "metadata": {},
   "source": [
    "### Deployment modes: Cluster and Client?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2b24f3",
   "metadata": {},
   "source": [
    "### How the client reads a file from HDFS ?\n",
    "\n",
    "<ul>\n",
    "    <li>HDFS Client: On user behalf, HDFS client interacts with NameNode and Datanode to fulfill user requests.</li>\n",
    "    <li>NameNode: NameNode is the master node that stores metadata about block locations, blocks of a file, etc. This metadata is used for file read and write operation.</li>\n",
    "    <li>DataNode: DataNodes are the slave nodes in HDFS. They store actual data (data blocks).</li>\n",
    "</ul>\n",
    " <p>HDFS client wants to read a file “File.txt”</p>\n",
    " <p>The client will reach out to NameNode asking locations of DataNodes containing data blocks. <br>\n",
    "    The NameNode first checks for required privileges, and if the client has sufficient privileges, the NameNode sends the locations of DataNodes containing blocks</p>\n",
    " <p> NameNode also gives a security token to the client, which they need to show to the DataNodes for authentication.</p>\n",
    " <a href=\"https://data-flair.training/blogs/hdfs-data-read-operation/\">How the client reads a file from HDFS Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d0dfb9",
   "metadata": {},
   "source": [
    "### What types of Hadoop nodes are used by HBase?\n",
    "Masters -- HDFS NameNode, YARN ResourceManager, and HBase Master. Slaves -- HDFS DataNodes, YARN NodeManagers, <br>\n",
    "and HBase RegionServers. The DataNodes, NodeManagers, and HBase RegionServers are co-located or co-deployed <br>\n",
    "for optimal data locality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290db283",
   "metadata": {},
   "source": [
    "### When using the default storage level for persist() what happens when an RDD does not fit in the cluster's memory?\n",
    "<p>If the RDD does not fit in memory, store the partitions on disk that don't fit in memory, and read them from there when they're needed.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510f066",
   "metadata": {},
   "source": [
    "### Spark OOM (Out of Memory ) Error — Closeup\n",
    "https://medium.com/swlh/spark-oom-error-closeup-462c7a01709d#:~:text=To%20fix%20this%20error%20we,size%20with%20below%20configuration%20setting.&text=GC%20Overhead%20limit%20exceeded.,if%20the%20value%20for%20spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd71349",
   "metadata": {},
   "source": [
    "### Python Spark Cumulative Sum by Group Using DataFrame\n",
    "\n",
    "https://stackoverflow.com/questions/45946349/python-spark-cumulative-sum-by-group-using-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a96dc9",
   "metadata": {},
   "source": [
    "### What is the Typical Process that you follow to optimize spark Job ?\n",
    "\n",
    "Melwin dont add everything what is there mention only those which you know well <br>\n",
    "https://www.xenonstack.com/blog/apache-spark-optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b677b0",
   "metadata": {},
   "source": [
    "## MCQ\n",
    "### Which property must a Spark structured streaming sink processess to ensure end-to-end exactly-once sematics ?\n",
    "<ul>\n",
    "    <li>Horizontally scalable</li>\n",
    "    <li>Predicatable</li>\n",
    "    <li>Cacheable</li>\n",
    "    <li>Idempotent</li>\n",
    "</ul>\n",
    "\n",
    "### Which of the following will ensure better performance of quries when using the Spark as a query engin ?\n",
    "<ul>\n",
    "    <li>Querying a large dataset</li>\n",
    "    <li>Including a surrogate primary key in the where clause</li>\n",
    "    <li>Increasing the amount of data skew on a join key</li>\n",
    "    <li>Including a filter on partition column in the where clause for predicate pushdown</li>\n",
    "</ul>\n",
    "\n",
    "### Which file format enables the use of predicate pushdown filtering as well as column pruning at the storage layer ?\n",
    "<ul>\n",
    "    <li>Parquet</li>\n",
    "    <li>Avro</li>\n",
    "    <li>CSV</li>\n",
    "    <li>JSON</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8399b04",
   "metadata": {},
   "source": [
    "## Example of Spark Submit cmd\n",
    "\n",
    "<pre>\n",
    "spark-submit --deploy-mode cluster --master yarn --driver-memory 10g \\\n",
    "--executor-memory 35g \\\n",
    "--executor-cores 5 \\\n",
    "--num-executors 12 \\ \n",
    "--jars s3://< s3-path > \\\n",
    "python_file.py 20190401 20190430\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664adad4",
   "metadata": {},
   "source": [
    "## Example of create spark session in Prod\n",
    "\n",
    "<pre>\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.InstanceProfileCredentialsProvider\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark.conf.set(\"credentials\", base64_string)  # this is to connect GCP account\n",
    "spark.conf.set(\"viewsEnabled\", \"true\")\n",
    "spark.conf.set(\"materializationDataset\", \"34532456245\")\n",
    "\n",
    "</pre>\n",
    "\n",
    "<pre>\n",
    "df = spark.read \\\n",
    "     .format(\"bigquery\") \\\n",
    "     .option(\"parentProject\", \"lively-encoder-854\") \\\n",
    "     .load(query)\n",
    "\n",
    "output_s3_path = \"< s3 path >\"\n",
    "\n",
    "df.coalesce(50).write.mode('append').parquet(output_s3_path)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed18cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
