{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f87c53ff",
   "metadata": {},
   "source": [
    "<pre>\n",
    "*****PySpark DF Question******\n",
    "Filename: covid19.txt\n",
    "1) Convert both the date columns to (yyyy-MM-dd) format.\n",
    "2) Have an extra column and populate the number of days between these two days.\n",
    "3) Partition the data by year columns and write to HDFS in parquet/avro/json format.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad23289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88834644",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"yettodecide\").master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc53906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "          .option(\"header\",True)\\\n",
    "          .option(\"InferSchema\",True)\\\n",
    "          .load(\"C:\\\\Personal\\\\Projects\\\\Spark\\\\dataset\\\\covid19.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da4a4e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+--------+----------+---+---+---+---+---------+---------+\n",
      "|D      |Y   |Dt        |Wkd     |CM        |C  |Com|TM |M  |V        |CL       |\n",
      "+-------+----+----------+--------+----------+---+---+---+---+---------+---------+\n",
      "|Exports|2015|31/01/2015|Saturday|01/02/2020|All|All|All|$  |257000000|257000000|\n",
      "|Exports|2015|01/02/2015|Sunday  |02/02/2020|All|All|All|$  |123000000|380000000|\n",
      "+-------+----+----------+--------+----------+---+---+---+---+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a0cc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,date_format, to_timestamp, datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a2ba9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Dt\", date_format(to_timestamp(col(\"Dt\"),\"dd/MM/yyyy\"), \"yyyy-MM-dd\") )\\\n",
    "  .withColumn(\"CM\", date_format(to_timestamp(col(\"CM\"),\"dd/MM/yyyy\"), \"yyyy-MM-dd\") )\\\n",
    "  .withColumn(\"DiffDate\" , datediff(col(\"CM\"),col(\"DT\")) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c5579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+---------+----------+---+---+---+---+---------+---------+--------+\n",
      "|      D|   Y|        Dt|      Wkd|        CM|  C|Com| TM|  M|        V|       CL|DiffDate|\n",
      "+-------+----+----------+---------+----------+---+---+---+---+---------+---------+--------+\n",
      "|Exports|2015|2015-01-31| Saturday|2020-02-01|All|All|All|  $|257000000|257000000|    1827|\n",
      "|Exports|2015|2015-02-01|   Sunday|2020-02-02|All|All|All|  $|123000000|380000000|    1827|\n",
      "|Exports|2015|2015-02-02|   Monday|2020-02-03|All|All|All|  $|176000000|556000000|    1827|\n",
      "|Exports|2015|2015-02-03|  Tuesday|2020-02-04|All|All|All|  $|115000000|671000000|    1827|\n",
      "|Exports|2015|2015-02-04|Wednesday|2020-02-05|All|All|All|  $| 74000000|746000000|    1827|\n",
      "+-------+----+----------+---------+----------+---+---+---+---+---------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0fedb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('json')\\\n",
    "        .option(\"header\",True)\\\n",
    "        .partitionBy(\"Y\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"C:\\\\Personal\\\\Projects\\\\Spark\\\\dataset\\\\5th_assignment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbe055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
