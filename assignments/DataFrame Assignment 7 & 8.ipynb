{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,date_format,datediff,to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"learning\").master('local').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****PySpark DF Question******\n",
    "## Filename: covid19.txt\n",
    "## 1) Convert both the date columns to (yyyy-MM-dd) format.\n",
    "## 2) Have an extra column and populate the number of days between these two days.\n",
    "## 3) Partition the data by year columns and write to HDFS in parquet/avro/json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- D: string (nullable = true)\n",
      " |-- Y: integer (nullable = true)\n",
      " |-- Dt: string (nullable = true)\n",
      " |-- Wkd: string (nullable = true)\n",
      " |-- CM: string (nullable = true)\n",
      " |-- C: string (nullable = true)\n",
      " |-- Com: string (nullable = true)\n",
      " |-- TM: string (nullable = true)\n",
      " |-- M: string (nullable = true)\n",
      " |-- V: integer (nullable = true)\n",
      " |-- CL: long (nullable = true)\n",
      "\n",
      "+-------+----+----------+---------+----------+---+---+---+---+---------+---------+\n",
      "|      D|   Y|        Dt|      Wkd|        CM|  C|Com| TM|  M|        V|       CL|\n",
      "+-------+----+----------+---------+----------+---+---+---+---+---------+---------+\n",
      "|Exports|2015|31/01/2015| Saturday|01/02/2020|All|All|All|  $|257000000|257000000|\n",
      "|Exports|2015|01/02/2015|   Sunday|02/02/2020|All|All|All|  $|123000000|380000000|\n",
      "|Exports|2015|02/02/2015|   Monday|03/02/2020|All|All|All|  $|176000000|556000000|\n",
      "|Exports|2015|03/02/2015|  Tuesday|04/02/2020|All|All|All|  $|115000000|671000000|\n",
      "|Exports|2015|04/02/2015|Wednesday|05/02/2020|All|All|All|  $| 74000000|746000000|\n",
      "+-------+----+----------+---------+----------+---+---+---+---+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df = spark.read.format('csv')\\\n",
    "                     .option('delimiter',',')\\\n",
    "                     .option('header','True')\\\n",
    "                     .option('inferSchema','True')\\\n",
    "                     .load('file:///home/saif/LFS/datasets/covid19.txt')\n",
    "covid_df.printSchema()\n",
    "covid_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Melwin please Note : is the Date is in strinf format we need to most likely to convert using \n",
    "## to_timestamp\n",
    "## Please refer the below example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+---+---+---+---+---------+---------+----------+----------+\n",
      "|      D|   Y|      Wkd|  C|Com| TM|  M|        V|       CL|        Dt|        CM|\n",
      "+-------+----+---------+---+---+---+---+---------+---------+----------+----------+\n",
      "|Exports|2015| Saturday|All|All|All|  $|257000000|257000000|2015-01-31|2020-02-01|\n",
      "|Exports|2015|   Sunday|All|All|All|  $|123000000|380000000|2015-02-01|2020-02-02|\n",
      "|Exports|2015|   Monday|All|All|All|  $|176000000|556000000|2015-02-02|2020-02-03|\n",
      "|Exports|2015|  Tuesday|All|All|All|  $|115000000|671000000|2015-02-03|2020-02-04|\n",
      "|Exports|2015|Wednesday|All|All|All|  $| 74000000|746000000|2015-02-04|2020-02-05|\n",
      "+-------+----+---------+---+---+---+---+---------+---------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df = covid_df.select(\n",
    "                col('D'),col('Y'),col('Wkd'),col('C'),col('Com'),col('TM'),col('M'),col('V'),col('CL'),\n",
    "                date_format(to_timestamp(col('Dt'),'dd/MM/yyyy'),'yyyy-MM-dd').alias('Dt'),\n",
    "                date_format(to_timestamp(col('CM'),'dd/MM/yyyy'),'yyyy-MM-dd').alias('CM')\n",
    "                )\n",
    "covid_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+---+---+---+---+---------+---------+----------+----------+--------+\n",
      "|      D|   Y|      Wkd|  C|Com| TM|  M|        V|       CL|        Dt|        CM|diffdate|\n",
      "+-------+----+---------+---+---+---+---+---------+---------+----------+----------+--------+\n",
      "|Exports|2015| Saturday|All|All|All|  $|257000000|257000000|2015-01-31|2020-02-01|    1827|\n",
      "|Exports|2015|   Sunday|All|All|All|  $|123000000|380000000|2015-02-01|2020-02-02|    1827|\n",
      "|Exports|2015|   Monday|All|All|All|  $|176000000|556000000|2015-02-02|2020-02-03|    1827|\n",
      "|Exports|2015|  Tuesday|All|All|All|  $|115000000|671000000|2015-02-03|2020-02-04|    1827|\n",
      "|Exports|2015|Wednesday|All|All|All|  $| 74000000|746000000|2015-02-04|2020-02-05|    1827|\n",
      "+-------+----+---------+---+---+---+---+---------+---------+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = covid_df.select(col('*'), \n",
    "    datediff(col('CM'),col('Dt')).alias('diffdate')\n",
    "                   )\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"Y\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .json(\"hdfs://localhost:9000/user/saif/HFS/Output/df_op/covid18_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "        .partitionBy(\"Y\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://localhost:9000/user/saif/HFS/Output/df_op/covid18_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\", True) \\\n",
    "        .partitionBy(\"Y\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"com.databricks.spark.avro\") \\\n",
    "        .save(\"hdfs://localhost:9000/user/saif/HFS/Output/df_op/covid18_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****PySpark DF Question*****\n",
    "## Filename: txns\n",
    "## Metadata: txnno: String, txndate: String, custno: String, amount: String, category: String, product: String, city: String, state: String, spendby: String\n",
    "## 1) Read the file & show 5 records.\n",
    "## 2) Separate txndate in date, month, year Column. Convert date column into words (e.g. 01 --> Sunday, 02 --> Monday).\n",
    "## 3) Find the sum of amount daywise.\n",
    "## 4) Write the output data in json format.\n",
    "## Output Columns:\n",
    "## Date(Words),Sum(Amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txnno: integer (nullable = true)\n",
      " |-- txndate: string (nullable = true)\n",
      " |-- custno: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- spendby: string (nullable = true)\n",
      "\n",
      "+-----+----------+-------+------+------------------+--------------------+-----------+----------+-------+\n",
      "|txnno|   txndate| custno|amount|          category|             product|       city|     state|spendby|\n",
      "+-----+----------+-------+------+------------------+--------------------+-----------+----------+-------+\n",
      "|    0|06-26-2011|4007024| 40.33|Exercise & Fitness|Cardio Machine Ac...|Clarksville| Tennessee| credit|\n",
      "|    1|05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves| Long Beach|California| credit|\n",
      "|    2|06-01-2011|4009775|  5.58|Exercise & Fitness|Weightlifting Mac...|    Anaheim|California| credit|\n",
      "|    3|06-05-2011|4002199|198.19|        Gymnastics|    Gymnastics Rings|  Milwaukee| Wisconsin| credit|\n",
      "|    4|12-17-2011|4002613| 98.81|       Team Sports|        Field Hockey|Nashville  | Tennessee| credit|\n",
      "+-----+----------+-------+------+------------------+--------------------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txn_df = spark.read.format('csv')\\\n",
    "              .option('delimiter',',')\\\n",
    "              .option('header','True')\\\n",
    "              .option('inferSchema','True')\\\n",
    "              .load('file:///home/saif/LFS/datasets/txns')\n",
    "txn_df.printSchema()\n",
    "txn_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month,col,to_timestamp,dayofmonth,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+---------+\n",
      "|   txndate|Year|month|day|  weekday|\n",
      "+----------+----+-----+---+---------+\n",
      "|06-26-2011|2011|    6| 26|   Sunday|\n",
      "|05-26-2011|2011|    5| 26| Thursday|\n",
      "|06-01-2011|2011|    6|  1|Wednesday|\n",
      "|06-05-2011|2011|    6|  5|   Sunday|\n",
      "|12-17-2011|2011|   12| 17| Saturday|\n",
      "+----------+----+-----+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txn_df.select(\n",
    "    col('txndate'),\n",
    "    year(to_timestamp(col('txndate'),'MM-dd-yyyy')).alias('Year'),\n",
    "    month(to_timestamp(col('txndate'),'MM-dd-yyyy')).alias('month'),\n",
    "    dayofmonth(to_timestamp(col('txndate'),'MM-dd-yyyy')).alias('day'),\n",
    "    date_format(to_timestamp(col('txndate'),'MM-dd-yyyy'),'EEEE').alias('weekday')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df= txn_df.select(\n",
    "    col('*'),\n",
    "    year(to_timestamp(col('txndate'),'MM-dd-yyyy')).alias('Year'),\n",
    "    month(to_timestamp(col('txndate'),'MM-dd-yyyy')).alias('month'),\n",
    "    dayofmonth(to_timestamp(col('txndate'),'MM-dd-yyyy')).alias('day'),\n",
    "    date_format(to_timestamp(col('txndate'),'MM-dd-yyyy'),'EEEE').alias('weekday')\n",
    ").groupby('weekday').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+------------------+---------+----------+--------+\n",
      "|  weekday|sum(txnno)|sum(custno)|       sum(amount)|sum(Year)|sum(month)|sum(day)|\n",
      "+---------+----------+-----------+------------------+---------+----------+--------+\n",
      "|Wednesday| 656193507|54651928480|1398153.0799999963| 27442106|     88416|  217559|\n",
      "|  Tuesday| 660684231|55289483879|1413094.4900000046| 27761855|     89376|  211615|\n",
      "|   Friday| 653784437|54640840104|1387614.2200000007| 27436073|     90435|  212183|\n",
      "| Thursday| 657094875|54924088380| 1410064.849999999| 27578854|     90044|  215351|\n",
      "| Saturday| 656714364|54668300900|1394970.8200000047| 27450150|     87696|  209580|\n",
      "+---------+----------+-----------+------------------+---------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txn_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df.write.option(\"header\",True) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .json(\"hdfs://localhost:9000/user/saif/HFS/Output/df_op/txns_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
