{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc98e9d-484f-4993-9458-2ee5d20a6456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/31 01:34:48 WARN Utils: Your hostname, de24, resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface enp0s3)\n",
      "25/07/31 01:34:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/31 01:34:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>196</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x769ffc3c1310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "conf = SparkConf().setAppName(\"196\").setMaster(\"local[4]\")\n",
    "spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c1c51-4eab-49ae-b3e7-91d8c9117f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Table: Person\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| email       | varchar |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains an email. The emails will not contain uppercase letters.\n",
    " \n",
    "\n",
    "Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id.\n",
    "\n",
    "For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.\n",
    "\n",
    "For Pandas users, please note that you are supposed to modify Person in place.\n",
    "\n",
    "After running your script, the answer shown is the Person table. \n",
    "The driver will first compile and run your piece of code and then show the Person table. \n",
    "The final order of the Person table does not matter.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Person table:\n",
    "+----+------------------+\n",
    "| id | email            |\n",
    "+----+------------------+\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "| 3  | john@example.com |\n",
    "+----+------------------+\n",
    "Output: \n",
    "+----+------------------+\n",
    "| id | email            |\n",
    "+----+------------------+\n",
    "| 1  | john@example.com |\n",
    "| 2  | bob@example.com  |\n",
    "+----+------------------+\n",
    "Explanation: john@example.com is repeated two times. We keep the row with the smallest Id = 1.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e299b6-16a5-404c-b5c7-16baf935d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "(1,'john@example.com'),\n",
    "(2,'bob@example.com' ),\n",
    "(3,'john@example.com')    \n",
    "]\n",
    "schema = ['id','email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bf7b09-deed-4a40-b80b-d11a23e6d2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "|  3|john@example.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0c2cb-4b87-4a71-add6-0c71cda84513",
   "metadata": {},
   "source": [
    "In PySpark DataFrames, we don’t have direct DELETE operations like traditional SQL. Instead, you filter out the rows you want to “delete” and overwrite or store the cleaned dataset. For your scenario, the logic is: keep only the rows whose id is the minimum for each email.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ab4ac07-8fbb-4400-879f-bb0d74686775",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "+---+----------------+\n",
      "\n",
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df.groupBy(F.col(\"email\"))\\\n",
    "         .agg(   F.min(F.col(\"id\")).alias(\"id\"),\n",
    "                 F.count(F.col(\"*\")).alias(\"count\")\n",
    "             )\\\n",
    "         .select(F.col(\"id\"),F.col(\"email\"))\n",
    "df_new.show()\n",
    "df = df_new\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64631606-2184-4cc3-b7bb-3fa1ebffc915",
   "metadata": {},
   "source": [
    "## Anothor Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c29af15-ef06-45b5-a9d6-fab59cba3c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|           email|\n",
      "+---+----------------+\n",
      "|  1|john@example.com|\n",
      "|  2| bob@example.com|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Get min(id) per email\n",
    "df_min_ids = df.groupBy(F.col(\"email\")).agg(F.min(F.col(\"id\")).alias(\"min_id\"))\n",
    "\n",
    "# Step 2: Join to original to retain full row\n",
    "df_clean = df.alias(\"t1\").join(df_min_ids.alias(\"t2\"), \n",
    "                               (F.col(\"t1.email\") == F.col(\"t2.email\")) & \n",
    "                               (F.col(\"t1.id\") == F.col(\"t2.min_id\")), \n",
    "                               \"inner\") \\\n",
    "             .select(F.col(\"t1.*\"))\n",
    "\n",
    "df_clean.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38209b3-f32b-404f-98aa-199ccdd6591c",
   "metadata": {},
   "source": [
    "## SQL Solution\n",
    "\n",
    "<pre>\n",
    "\n",
    "DELETE FROM Person \n",
    "    WHERE id NOT IN (\n",
    "        SELECT x.id from (\n",
    "            SELECT email, min(id) as id \n",
    "            FROM Person\n",
    "            GROUP BY email\n",
    "            HAVING count(*) >= 1    \n",
    "        ) x\n",
    "    );\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9da43d-a975-48d8-83fb-4f5e489d1a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
