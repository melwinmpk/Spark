{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95dff9c5",
   "metadata": {},
   "source": [
    "<h1>PySpark</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf993d",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>PySpark is a Python API for Apache Spark to process larger datasets in a\n",
    "distributed cluster. <br>It is written in Python to run a Python application using\n",
    "Apache Spark capabilities</li>\n",
    "    <li>Spark basically is written in Scala, and due to its adaptation in industry, <br> its\n",
    "equivalent PySpark API has been released for Python Py4J. </li>\n",
    "    <li><b>Py4J</b> is a Java library that is integrated within Spark and allows python to\n",
    "dynamically interface with JVM objects, <br>hence to run PySpark you also need Java\n",
    "to be installed along with Python, and Apache Spark. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c504a4",
   "metadata": {},
   "source": [
    "<h2>Features of Spark</h2>\n",
    "<ol>\n",
    "    <li>PySpark RDD (pyspark.RDD)</li>\n",
    "    <li>PySpark DataFrame and SQL (pyspark.sql)</li>\n",
    "    <li>PySpark Streaming (pyspark.streaming)</li>\n",
    "    <li>PySpark MLib (pyspark.ml, pyspark.mllib)</li>\n",
    "    <li>PySpark GraphFrames (GraphFrames)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ebfbc",
   "metadata": {},
   "source": [
    "<h3>How to create SparkContext using SparkConf ?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9348dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages that must be Imported\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f819320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conf object\n",
    "# setAppNAme should be the relevent one based on the Program\n",
    "sparkConf = SparkConf( ) \\\n",
    " .setAppName(\"WordCount\") \\\n",
    " .setMaster(\"local\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217a18f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext object \n",
    "sc = SparkContext(conf=sparkConf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19101a02",
   "metadata": {},
   "source": [
    "Note: <b>Only one SparkContext</b> may be active per <b>JVM</b>. You must stop the active one before\n",
    "creating a new one as shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639d225a",
   "metadata": {},
   "source": [
    "<h3>How to create SparkSession?</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a8f7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d60c1c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"WordCount\")\\\n",
    "        .master(\"local[3]\")\\\n",
    "        .getOrCreate()\n",
    "# spark.sparkContext( ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba048e5",
   "metadata": {},
   "source": [
    "<h3>What is the setMaster in SparkContext and SparkSession Code?</h3>\n",
    "<p><b>setMaster(String master)</b> The master URL to connect to,<br> \n",
    "      such as \"local\" to run locally with one thread,<br> \"local[4]\" to run locally with 4 cores,<br>\n",
    "    or \"spark://master:7077\" to run on a Spark standalone cluster.<br> SparkConf. setSparkHome(String home) Set the location where Spark is installed on worker nodes.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6db8d",
   "metadata": {},
   "source": [
    "<h3>What does getOrCreate do in Spark?</h3>\n",
    "<p>Within the same JVM, getOrCreate() will give you the same instance of SparkContext;<br>\n",
    "   and this will help you share broadcast variables,<br> \n",
    "   etc among different applications spawned by the same Spark Driver.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d034464",
   "metadata": {},
   "source": [
    "<h3>Read data from text file in RDD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c7635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "readFile = sc.textFile(\"hdfs://localhost:9000/user/saif/HFS/Input/wordcount.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b6af1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(readFile) => <class 'pyspark.rdd.RDD'>\n",
      "type(readFile.collect()) => <class 'list'>\n",
      "\n",
      "output=>\n",
      "Saif Ram Ram Ram Saif\n",
      "Mitali Manas Mitali Mitali\n",
      "Pramod Shravan Shravan\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(readFile) => {type(readFile)}\")\n",
    "print(f\"type(readFile.collect()) => {type(readFile.collect())}\")\n",
    "print(\"\\noutput=>\")\n",
    "for i in readFile.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f404ba3",
   "metadata": {},
   "source": [
    "<h3>what does the collect() do?</h3>\n",
    "<p>Spark collect() and collectAsList() are <b>action operation</b> that is used to retrieve all the elements of the <b>RDD/DataFrame/Dataset</b> (from all nodes)<br> <b>to the driver node</b>.<br> We should use the collect() on smaller dataset usually after filter(), group(), count() e.t.c. <br>Retrieving on larger dataset results in out of memory.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0945a8",
   "metadata": {},
   "source": [
    "<h3> Split each line into words from the same file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7b44ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saif\n",
      "Ram\n",
      "Ram\n",
      "Ram\n",
      "Saif\n"
     ]
    }
   ],
   "source": [
    "splitWords = readFile.flatMap(lambda line: line.split(\" \"))\n",
    "splitWords.collect()\n",
    "for i in splitWords.take(5):\n",
    "    print(i) \n",
    "    \n",
    "# take(N) is used to get a \"N\" no or records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa6556d",
   "metadata": {},
   "source": [
    "<h3>Assign the word with Value as 1</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fedfd1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Saif', 1)\n",
      "('Ram', 1)\n",
      "('Ram', 1)\n",
      "('Ram', 1)\n",
      "('Saif', 1)\n",
      "('Mitali', 1)\n",
      "('Manas', 1)\n",
      "('Mitali', 1)\n",
      "('Mitali', 1)\n",
      "('Pramod', 1)\n",
      "('Shravan', 1)\n",
      "('Shravan', 1)\n"
     ]
    }
   ],
   "source": [
    "wordAssign = splitWords.map(lambda word: (word, 1)) \n",
    "for i in wordAssign.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51608ee",
   "metadata": {},
   "source": [
    "<h3> Count the occurrence of each word</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7918ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Saif', 2)\n",
      "('Ram', 3)\n",
      "('Mitali', 3)\n",
      "('Manas', 1)\n",
      "('Pramod', 1)\n",
      "('Shravan', 2)\n"
     ]
    }
   ],
   "source": [
    "wordCount = wordAssign.reduceByKey(lambda a,b:a+b)\n",
    "for i in wordCount.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899bb01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a1449b",
   "metadata": {},
   "source": [
    "<h3>Complete Word Count Program</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb12b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Saif', 2)\n",
      "('Ram', 3)\n",
      "('Mitali', 3)\n",
      "('Manas', 1)\n",
      "('Pramod', 1)\n",
      "('Shravan', 2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sparkConf = SparkConf( ) \\\n",
    " .setAppName(\"WordCount\") \\\n",
    " .setMaster(\"local\") \n",
    "\n",
    "sc = SparkContext(conf=sparkConf)\n",
    "\n",
    "readFile = sc.textFile(\"hdfs://localhost:9000/user/saif/HFS/Input/wordcount.txt\")\n",
    "\n",
    "splitWords = readFile.flatMap(lambda line: line.split(\" \"))\n",
    "wordAssign = splitWords.map(lambda word: (word, 1)) \n",
    "\n",
    "wordCount = wordAssign.reduceByKey(lambda a,b:a+b)\n",
    "for i in wordCount.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477a61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
