{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating the RDD's</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>There are three ways to create RDDs in Spark</b>\n",
    "<ul>\n",
    "       <li>Parallelizing via collections in driver program.</li>\n",
    "       <li>Creating a dataset in an external storage system (e.g. HDFS, HBase, and Shared FS).</li>\n",
    "       <li>Creating RDD from existing RDDs.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parallelizing via collections in driver program</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages that must be Imported\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf( ) \\\n",
    " .setAppName(\"Checking\") \\\n",
    " .setMaster(\"local\") \n",
    "sc = SparkContext(conf=sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName(\"WordCount\")\\\n",
    "        .master(\"local[3]\")\\\n",
    "        .getOrCreate()\n",
    "# spark.sparkContext( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = sc.parallelize(data)\n",
    "rdd.collect( )\n",
    "print (rdd.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is parallelized in spark</h3>\n",
    "<p>Parallelize is a <b>method to create an RDD from an existing collection (For e.g Array)</b>\n",
    "    present in the driver.<br> The elements present in the collection are copied to form a distributed dataset on which we can operate on in parallel.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create RDD with partition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partition Count:1\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10, 11, 12]]\n",
      "After changing Partition Count:5\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = sc.parallelize(data)\n",
    "print(\"Initial Partition Count:\"+str(rdd.getNumPartitions()))\n",
    "partition_data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "partition_rdd = spark.sparkContext.parallelize(partition_data, 5)\n",
    "print(partition_rdd.collect())\n",
    "print(partition_rdd.glom().collect())\n",
    "print(\"After changing Partition Count:\"+str(partition_rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is the Glom usef for ?\n",
    "<p>Return an RDD created by coalescing all elements within each partition into a list.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create RDD from external text file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id,name,city', '101,saif,mumbai', '102,mitali,pune', '103,ram,balewadi']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readFile = sc.textFile(\"file:///home/saif/LFS/datasets/emp.txt\")\n",
    "readFile.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating RDD from existing RDD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Partition Count:1\n",
      "Re-partition count:4\n",
      "Re-partition count:3\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Partition Count:\"+str(readFile.getNumPartitions()))\n",
    "repartition_Rdd = readFile.repartition(4)\n",
    "print(\"Re-partition count:\"+str(repartition_Rdd.getNumPartitions()))\n",
    "coalesce_Rdd = readFile.repartition(3)\n",
    "print(\"Re-partition count:\"+str(coalesce_Rdd.getNumPartitions())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Map</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)(2, 4)(3, 5)(4, 6)(5, 7)"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5])\n",
    "result = a.map(lambda x: (x, x+2))\n",
    "result.collect()\n",
    "for element in result.collect():\n",
    "    print(element, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap \n",
    "<p>Returns a new RDD by first applying a function to all elements of this RDD, and then\n",
    "flattening the results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5])\n",
    "result = a.flatMap(lambda x: (x, x**2))\n",
    "result.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "result = a.filter(lambda x: x % 2 == 0)\n",
    "print(result.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 1, 2, 2, 3, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5])\n",
    "b = sc.parallelize([1,2,2,3,3])\n",
    "a.union(b).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5])\n",
    "b = sc.parallelize([1,2,2,3,3])\n",
    "a.intersection(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,2,3,4,4,4,5])\n",
    "a.distinct( ).collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', <pyspark.resultiterable.ResultIterable object at 0x7fa4f7a377c0>), ('B', <pyspark.resultiterable.ResultIterable object at 0x7fa4f7acaee0>)]\n",
      "2\n",
      "1\n",
      "1\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('A', 2), ('B', 1), ('B', 5), ('A', 1), ('B', 10)])\n",
    "result = x.groupByKey()\n",
    "print(result.collect())\n",
    "for j in result.collect():\n",
    "    for i in j[1]:\n",
    "        print(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D81l3yBTUWAA&psig=AOvVaw3HJKc_N3SDAiXYf2CftEtp&ust=1642398686918000&source=images&cd=vfe&ved=0CAgQjRxqFwoTCMiN_ITKtfUCFQAAAAAdAAAAABAD\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Saif', 1), ('Ram', 3), ('Mitali', 1), ('Aniket', 2)]\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize([\"Saif\", \"Ram\", \"Mitali\", \"Aniket\", \"Ram\", \"Ram\", \"Aniket\"])\n",
    "wordCount = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a + b)\n",
    "print(wordCount.collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spark difference between reduceByKey vs. groupByKey vs. aggregateByKey vs. combineByKey</h3>\n",
    "<a href=\"https://stackoverflow.com/questions/43364432/spark-difference-between-reducebykey-vs-groupbykey-vs-aggregatebykey-vs-combi#:~:text=groupByKey%20can%20cause%20out%20of,collected%20on%20the%20reduced%20workers.&text=Data%20are%20combined%20at%20each,with%20the%20exact%20same%20type\" target=\"_blank\">LINK</a>\n",
    "<h3>Is groupByKey ever preferred over reduceByKey ?</h3>\n",
    "<a href=\"https://stackoverflow.com/questions/33221713/is-groupbykey-ever-preferred-over-reducebykey\" target=\"_blank\">Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aniket', 2), ('Mitali', 1), ('Ram', 3), ('Saif', 1)]\n",
      "[('Saif', 1), ('Ram', 3), ('Mitali', 1), ('Aniket', 2)]\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize([\"Saif\", \"Ram\", \"Mitali\", \"Aniket\", \"Ram\", \"Ram\", \"Aniket\"])\n",
    "wordCount = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a + b)\n",
    "print(wordCount.sortByKey().collect())\n",
    "print(wordCount.sortByKey(False).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Join</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([('C', 4), ('B', 3), ('A', 2), ('A', 1)])\n",
    "b = sc.parallelize([('A', 8), ('B', 7), ('A', 6), ('D', 5)])\n",
    "print(a.join(b).collect()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Types of joins:</h3>\n",
    "<ul>\n",
    "    <li>join</li>\n",
    "    <li>leftOuterJoin</li>\n",
    "    <li>rightOuterJoin</li>\n",
    "    <li>fullOuterJoin</li>\n",
    "    <li>Cartesian</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Coalesce</h3>\n",
    "<ul>\n",
    "<li>In coalesce ( ) we use existing partition so that less data is shuffled.<br> To avoid full\n",
    "shuffling of data we use coalesce ( ) function.<br> Using this we can reduce the number\n",
    "of the partition.</li>\n",
    "<li>Creates unequal sized partitions.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Repartition</h3>\n",
    "<ul>\n",
    "    <li>Used to increase or decrease the number of partitions.</li>\n",
    "    <li>A network shuffle will be triggered which can increase data movement.</li>\n",
    "    <li>Creates equal sized partitions.</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n",
      "After Coalesce: 1\n",
      "After Repartition: 5\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "b = a.getNumPartitions()\n",
    "print(b)\n",
    "c = a.glom().collect()\n",
    "print(c) \n",
    "d = a.coalesce(1)\n",
    "print(\"After Coalesce: \"+str(d.getNumPartitions()))\n",
    "e = a.repartition(5)\n",
    "print(\"After Repartition: \"+str(e.getNumPartitions())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Action Operations</h2>\n",
    "<ul>\n",
    "    <li>count</li>\n",
    "    <li>take</li>\n",
    "    <li>collect</li>\n",
    "    <li>top</li>\n",
    "    <li>countByValue</li>\n",
    "    <li>countByKey</li>\n",
    "    <li>reduce</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "a.count( ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.take(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.top(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 2, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([\"Saif\", \"Mitali\", \"Ram\", \"Ram\", \"Ram\", \"Mitali\"])\n",
    "a.countByValue()\n",
    "a.countByValue().keys()\n",
    "a.countByValue().values() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([2, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([('A', 2), ('A', 1), ('C',1), ('B',5)])\n",
    "x.countByKey()\n",
    "x.countByKey().values() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "a.reduce(lambda a, b: a + b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
