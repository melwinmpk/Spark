{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Frame<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf =  SparkConf().setAppName(\"Trying\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Trying\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convert the Rdd's to DF</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id,name,city', '101,saif,mumbai', '102,mitali,pune', '103,ram,balewadi']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame from Data sources\n",
    "rdd1 = sc.textFile(\"file:///home/saif/LFS/datasets/emp.txt\")\n",
    "print(rdd1.collect())\n",
    "type(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+---+------+--------+\n",
      "|id |name  |city    |\n",
      "+---+------+--------+\n",
      "|101|saif  |mumbai  |\n",
      "|102|mitali|pune    |\n",
      "|103|ram   |balewadi|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using toDF()\n",
    "rdd1 = rdd1.filter(lambda x: x != 'id,name,city' )\n",
    "rdd1 = rdd1.map(lambda x: x.split(','))\n",
    "df = rdd1.toDF(['id','name','city'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept: string (nullable = true)\n",
      " |-- deptno: long (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|dept     |deptno|\n",
      "+---------+------+\n",
      "|Finance  |10    |\n",
      "|Marketing|20    |\n",
      "|Sales    |30    |\n",
      "|IT       |40    |\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\", 10),\n",
    "            (\"Marketing\", 20),\n",
    "            (\"Sales\", 30),\n",
    "            (\"IT\", 40)]\n",
    "rdd = sc.parallelize(dept)\n",
    "df = rdd.toDF([\"dept\", \"deptno\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using createDataFramee(data=, schema = ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept: string (nullable = true)\n",
      " |-- deptno: long (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|dept     |deptno|\n",
      "+---------+------+\n",
      "|Finance  |10    |\n",
      "|Marketing|20    |\n",
      "|Sales    |30    |\n",
      "|IT       |40    |\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptdata = [(\"Finance\", 10),\n",
    "            (\"Marketing\", 20),\n",
    "            (\"Sales\", 30),\n",
    "            (\"IT\", 40)]\n",
    "dataSchema = [\"dept\", \"deptno\"]\n",
    "df = spark.createDataFrame(data=deptdata, schema = dataSchema) \n",
    "df.printSchema()\n",
    "df.show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading the df from the source</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|id |name,sal|country|\n",
      "+---+--------+-------+\n",
      "|101|sohail  |1000   |\n",
      "|102|Saif    |2000   |\n",
      "|103|Mitali  |3000   |\n",
      "|104|Manas   |4000   |\n",
      "|105|Ram     |5000   |\n",
      "+---+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name,sal: string (nullable = true)\n",
      " |-- country: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "            .option('delimiter','|') \\\n",
    "            .option('header', 'True') \\\n",
    "            .option('inferSchema', 'True') \\\n",
    "            .load('file:///home/saif/LFS/datasets/emp_all.txt')\n",
    "# for HDFS hdfs://localhost:9000/user/saif/HFS/Output/....\n",
    "df.show(5,truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the DF to a file <br><br> Write the df </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('csv')\\\n",
    "        .mode('append')\\\n",
    "        .save('file:///home/saif/LFS/datasets/emp_all_write.txt')\n",
    "# for HDFS hdfs://localhost:9000/user/saif/HFS/Output/...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving modes:</h3>\n",
    "<b>PySpark DataFrameWriter also has a method mode ( ) to specify saving mode.</b>\n",
    "<ul>\n",
    "    <li><b>error:</b> This is a default option when the file already exists, it returns an error.</li>\n",
    "    <li><b>ignore:</b> Ignores write operation when the file already exists.</li>\n",
    "    <li><b>append:</b> To add the data to the existing file.</li>\n",
    "    <li><b>overwrite:</b> This mode is used to overwrite the existing file.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Select</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single & Multiple Columns:\n",
    "df.select(\"firstname\").show()\n",
    "df.select(\"firstname\", \"lastname\").show()\n",
    "# Using Dataframe object name:\n",
    "df.select(df.firstname, df.lastname).show()\n",
    "# Using col function:\n",
    "df.select(col(\"firstname\"), col(\"lastname\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melwin you shouls look into \n",
    "## DataFrame Partitions & Executors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>*****Spark Jobs, Stages & Tasks *******</h2>\n",
    "<a href=\"https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20192582#content\">link Udemy (login with your account)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **** Data Frame Partitions and Executors *****\n",
    "<a href=\"https://www.udemy.com/course/apache-spark-programming-in-python-for-beginners/learn/lecture/20192576#content\">link Udemy (login with your account)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melwin Remember every action has at least one Job and every job has at least one stage and each stage has at least one task\n",
    "### Task is depends on the Partition if the n is the partition then n*task will be there for the respective Job\n",
    "### Look at the video to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 8], [5], [4, 8], [10]]\n"
     ]
    }
   ],
   "source": [
    "def add_to_list(item,list_param=[]):\n",
    "    list_param.append(item)\n",
    "    return list_param\n",
    "\n",
    "def add_to_list2(item,list_param=None):\n",
    "    if not list_param:\n",
    "        list_param=[] #\n",
    "    list_param.append(item)\n",
    "    return list_param\n",
    " \n",
    "master=list()\n",
    "master.append(add_to_list(4))\n",
    "master.append(add_to_list2(5))\n",
    "master.append(add_to_list(8))\n",
    "master.append(add_to_list2(10))\n",
    "\n",
    "print(master)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
