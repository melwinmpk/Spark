{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef00dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17612cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf =  SparkConf().setAppName(\"Trying\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=sparkConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1148c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .appName('yettodecide')\\\n",
    "        .master('local')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd49ae",
   "metadata": {},
   "source": [
    "### Date/Time Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084c7aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  start_dt|    end_dt|\n",
      "+----------+----------+\n",
      "|2021-01-15|2021-02-15|\n",
      "+----------+----------+\n",
      "\n",
      "root\n",
      " |-- start_dt: string (nullable = true)\n",
      " |-- end_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.createDataFrame([('2021-01-15', '2021-02-15',)], ['start_dt', 'end_dt'])\n",
    "df_1.show()\n",
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192013fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|  start_dt|    end_dt| dt_format|\n",
      "+----------+----------+----------+\n",
      "|2021-01-15|2021-02-15|15/01/2021|\n",
      "+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- start_dt: string (nullable = true)\n",
      " |-- end_dt: string (nullable = true)\n",
      " |-- dt_format: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "df_2 = df_1.select(\"start_dt\", \"end_dt\", date_format(\"start_dt\",'dd/MM/yyyy').alias(\"dt_format\"))\n",
    "df_2.show()\n",
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f635f1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|  start_dt|    end_dt|    cur_dt|\n",
      "+----------+----------+----------+\n",
      "|2021-01-15|2021-02-15|2022-01-27|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|  start_dt|    end_dt|add_2_days|\n",
      "+----------+----------+----------+\n",
      "|2021-01-15|2021-02-15|2021-01-17|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|  start_dt|    end_dt|sub_2_days|\n",
      "+----------+----------+----------+\n",
      "|2021-01-15|2021-02-15|2021-01-13|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+----------+-----------+\n",
      "|  start_dt|    end_dt|sub_2_dates|\n",
      "+----------+----------+-----------+\n",
      "|2021-01-15|2021-02-15|         31|\n",
      "+----------+----------+-----------+\n",
      "\n",
      "+----------+----------+------------+\n",
      "|  start_dt|    end_dt|add_2_months|\n",
      "+----------+----------+------------+\n",
      "|2021-01-15|2021-02-15|  2021-03-15|\n",
      "+----------+----------+------------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|  start_dt|    end_dt| add_2_Yrs|\n",
      "+----------+----------+----------+\n",
      "|2021-01-15|2021-02-15|2023-01-15|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, date_add , date_sub, datediff, add_months\n",
    "\n",
    "df_2.select(\"start_dt\", \"end_dt\", current_date().alias(\"cur_dt\")).show()\n",
    "df_2.select(\"start_dt\", \"end_dt\", date_add(\"start_dt\", 2).alias(\"add_2_days\")).show() \n",
    "df_2.select(\"start_dt\", \"end_dt\", date_sub(\"start_dt\", 2).alias(\"sub_2_days\")).show() \n",
    "df_2.select(\"start_dt\", \"end_dt\", datediff(\"end_dt\", \"start_dt\").alias(\"sub_2_dates\")).show()\n",
    "df_2.select(\"start_dt\", \"end_dt\", add_months(\"start_dt\", 2).alias(\"add_2_months\")).show()\n",
    "df_2.select(\"start_dt\", \"end_dt\", add_months(\"start_dt\", 2 * 12).alias(\"add_2_Yrs\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "592c19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, weekofyear, dayofweek, dayofyear,last_day,months_between,next_day,quarter,trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b9c61fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+-----+---+------------+-----------+-----------+\n",
      "|  start_dt|    end_dt|Year|Month|Day|Week_of_Year|Day_of_Week|Day_of_Year|\n",
      "+----------+----------+----+-----+---+------------+-----------+-----------+\n",
      "|2021-01-15|2021-02-15|2021|    1| 15|           2|          6|         15|\n",
      "+----------+----------+----+-----+---+------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.select(\"start_dt\", \"end_dt\", year(\"start_dt\").alias(\"Year\")\n",
    " , month(\"start_dt\").alias(\"Month\")\n",
    " , dayofmonth(\"start_dt\").alias(\"Day\")\n",
    " , weekofyear(\"start_dt\").alias(\"Week_of_Year\")\n",
    " , dayofweek(\"start_dt\").alias(\"Day_of_Week\")\n",
    " , dayofyear(\"start_dt\").alias(\"Day_of_Year\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35429210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|  start_dt|    end_dt|  Last_Day|\n",
      "+----------+----------+----------+\n",
      "|2021-01-15|2021-02-15|2021-01-31|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.select(\"start_dt\", \"end_dt\", last_day(\"start_dt\").alias(\"Last_Day\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d76ccca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+\n",
      "|  start_dt|    end_dt|Months_Betwn|\n",
      "+----------+----------+------------+\n",
      "|2021-01-15|2021-02-15|         1.0|\n",
      "+----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.select(\"start_dt\", \"end_dt\",\n",
    " months_between(\"end_dt\", \"start_dt\").alias(\"Months_Betwn\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f899085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|  start_dt|    end_dt|Next_Monday|\n",
      "+----------+----------+-----------+\n",
      "|2021-01-15|2021-02-15| 2021-01-18|\n",
      "+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.select(\"start_dt\", \"end_dt\", next_day(\"start_dt\", \"Mon\").alias(\"Next_Monday\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b2514ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+\n",
      "|  start_dt|    end_dt|Quarter_of_Year|\n",
      "+----------+----------+---------------+\n",
      "|2021-01-15|2021-02-15|              1|\n",
      "+----------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.select(\"start_dt\", \"end_dt\", quarter(\"start_dt\").alias(\"Quarter_of_Year\")).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "87d856f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+\n",
      "|  start_dt|    end_dt|Trunc_Year|Trunc_Month|\n",
      "+----------+----------+----------+-----------+\n",
      "|2021-01-15|2021-02-15|2021-01-01| 2021-02-01|\n",
      "+----------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " ### Truncate Date to Year, Month\n",
    "df_2.select(\"start_dt\", \"end_dt\", trunc(\"start_dt\", \"year\").alias(\"Trunc_Year\"),\n",
    "trunc(\"end_dt\", \"month\").alias(\"Trunc_Month\")).show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b152899",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c2d5172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(start_dt='2021-01-15')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.select(df_2[\"start_dt\"]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ac1ec",
   "metadata": {},
   "source": [
    "#### approx_count_distinct()\n",
    " function returns the count of distinct items in a group<br>\n",
    "approxDistinctCount = agg_df.select(approx_count_distinct(\"salary\")) <br>\n",
    "approxDistinctCount.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0e089",
   "metadata": {},
   "source": [
    "#### avg (average)\n",
    "avgSal = agg_df.select(avg(\"salary\"))  #  sum, min, max <br>\n",
    "avgSal.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8c43c0",
   "metadata": {},
   "source": [
    "### collect_list\n",
    "function returns all values from an input column with duplicates <br>\n",
    "agg_df.select(collect_list(\"salary\")).show(truncate=False)  <br>\n",
    "\n",
    "----\n",
    "\n",
    "### collect_set\n",
    "collect_set ( ) function returns all values from an input column with NO duplicate\n",
    "values.<br>\n",
    "\n",
    "agg_df.select(collect_set(\"salary\")).show(truncate=False) \n",
    "\n",
    "----\n",
    "\n",
    "### countDistinct\n",
    "countDistinct ( ) function returns the number of distinct elements in a columns. \n",
    "\n",
    "df2 = agg_df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False) \n",
    "\n",
    "----\n",
    "\n",
    "### count\n",
    "cnt = agg_df.count()<br>\n",
    "print(cnt) \n",
    "\n",
    "----\n",
    "\n",
    "###  first/last\n",
    "\n",
    "\n",
    "agg_df.select(first(\"salary\")).show(truncate=False)<br>\n",
    "agg_df.select(last(\"salary\")).show(truncate=False) \n",
    "\n",
    "----\n",
    "\n",
    "### sumDistinct\n",
    "\n",
    "agg_df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f8918",
   "metadata": {},
   "source": [
    "## Windows Function\n",
    "\n",
    "<ul>\n",
    "    <li>ranking functions</li>\n",
    "    <li>analytic functions</li>\n",
    "    <li>aggregate functions</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5abbb9",
   "metadata": {},
   "source": [
    "<pre>\n",
    "    windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\") \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca1b7fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff37328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae6e16da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row_number\n",
    "\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)).show(truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1569d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rank\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08d46095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank\n",
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4b6fac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ntile\n",
    "from pyspark.sql.functions import ntile\n",
    "\n",
    "\n",
    "df.withColumn(\"ntile\", ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52bea187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n",
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lag\n",
    "from pyspark.sql.functions import lag,lead\n",
    "\n",
    "df.withColumn(\"lag\", lag(\"salary\", 2).over(windowSpec)).show()\n",
    "df.withColumn(\"lead\", lead(\"salary\", 2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601f072",
   "metadata": {},
   "source": [
    "## Window Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68e1169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg,col,sum,min,max\n",
    "\n",
    "windowSpecAgg = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\", row_number().over(windowSpec)) \\\n",
    " .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    " .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    " .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    " .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    " .where(col(\"row\") == 1).select(\"department\", \"avg\", \"sum\", \"min\", \"max\") \\\n",
    " .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823cd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
