{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdc92dc",
   "metadata": {},
   "source": [
    "## Cache & Persist\n",
    "<ul>\n",
    "    <li>Using cache ( ) and persist ( ) methods, Spark provides an optimization mechanism to\n",
    "store the intermediate computation of a Spark DataFrame so they can be reused in\n",
    "subsequent actions.</li>\n",
    "    <li>When you persist data, each node stores its partitioned data in memory and reuses\n",
    "them in other actions on that dataset.</li>\n",
    "    <li>Sparkâ€™s persisted data on nodes are fault-tolerant meaning if any partition of a Dataset\n",
    "is lost, it will automatically be recomputed using the original transformations that\n",
    "created it.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb8aa",
   "metadata": {},
   "source": [
    "## Cache\n",
    "### Syntax\n",
    "\n",
    "<pre>rdd.cache( )</pre>\n",
    "<pre>df.cache( )</pre>\n",
    "\n",
    "### To check whether the dataframe is cached or not\n",
    "\n",
    "<pre>df.is_cached</pre>\n",
    "<pre>df.storageLevel.useMemory</pre>\n",
    "\n",
    "<p>both the output gives the booliean values <b>True</b> or <b>False</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add3a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22be000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Yesttodecide\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1ab752",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDF = spark.read.format(\"csv\")\\\n",
    "        .options(header=\"true\", inferSchema=\"true\")\\\n",
    "        .load('C:\\\\Personal\\\\Projects\\\\Spark\\\\dataset\\\\txns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "582a93e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+------+------------------+---------------------------------+-----------+----------+-------+\n",
      "|txnno|txndate   |custno |amount|category          |product                          |city       |state     |spendby|\n",
      "+-----+----------+-------+------+------------------+---------------------------------+-----------+----------+-------+\n",
      "|0    |06-26-2011|4007024|40.33 |Exercise & Fitness|Cardio Machine Accessories       |Clarksville|Tennessee |credit |\n",
      "|1    |05-26-2011|4006742|198.44|Exercise & Fitness|Weightlifting Gloves             |Long Beach |California|credit |\n",
      "|2    |06-01-2011|4009775|5.58  |Exercise & Fitness|Weightlifting Machine Accessories|Anaheim    |California|credit |\n",
      "|3    |06-05-2011|4002199|198.19|Gymnastics        |Gymnastics Rings                 |Milwaukee  |Wisconsin |credit |\n",
      "|4    |12-17-2011|4002613|98.81 |Team Sports       |Field Hockey                     |Nashville  |Tennessee |credit |\n",
      "+-----+----------+-------+------+------------------+---------------------------------+-----------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDF.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b53436",
   "metadata": {},
   "source": [
    "### Catch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ebbf10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDF.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4203730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacheDf = myDF.where(col(\"state\") == \"California\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a014f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13035\n"
     ]
    }
   ],
   "source": [
    "print(cacheDf.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caada0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cacheDf.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef854ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11257\n"
     ]
    }
   ],
   "source": [
    "readingCachedDf = cacheDf.where(col(\"spendby\") == \"credit\")\n",
    "print(readingCachedDf.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778f094",
   "metadata": {},
   "source": [
    "## Persist\n",
    "<ul>\n",
    "    <li>Persist ( ) in Apache Spark by default takes the storage level as MEMORY_AND_DISK to\n",
    "save the Spark dataframe and RDD.</li>\n",
    "    <li>Using persist ( ), will initially start storing the data in JVM memory and when the data\n",
    "requires additional storage to accommodate, it pushes some excess data in the\n",
    "partition to disk and reads back the data from disk when it is required.</li>\n",
    "    <li>Since it involves I/O operation, persist ( ) is considerably slower than cache ( ).</li>\n",
    "    <li>We have many options of storage levels that can be used with persist ( ).</li>\n",
    "</ul>\n",
    "\n",
    "### Syntax\n",
    "<pre>\n",
    "#persist dataframe with default storage-level\n",
    "df.persist( )\n",
    "\n",
    "#persist dataframe with MEMORY_AND_DISK_2\n",
    "df.persist(pyspark.StorageLevel.MEMORY_AND_DISK_2)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9187c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "persistDf = myDF.persist(StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e16b1f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95904"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "# df.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "# df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "# df.persist(pyspark.StorageLevel.MEMORY_ONLY_SER)\n",
    "# df.persist(pyspark.StorageLevel.MEMORY_AND_DISK_SER)\n",
    "# df.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0934b5c",
   "metadata": {},
   "source": [
    "### Spark RDD Unpersist\n",
    "<pre>\n",
    "df.unpersist ( )\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ed693",
   "metadata": {},
   "source": [
    "### Difference between Cache and Persist\n",
    "\n",
    "https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/#:~:text=Both%20caching%20and%20persisting%20are,the%20user%2Ddefined%20storage%20level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e10f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
